{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation Bayesian Independent Component Analysis (vbICA)\n",
    "\n",
    "The following discussion is based on Gualandi et al. (2015). \n",
    "\n",
    "This method should be ideal for determining transient signals in the borehole strainmeter data. A great thesis (Choudrey, 2002) is available describing the development of the method, suitable for any incoming grad student to dive into as long as they have a basic understanding of calculus.... fair warning, it is 250 pages but has a fun tone to it (at least for the parts I read). \n",
    "\n",
    "One assumption in vbICA, which is ideal for solving the classic Blind Source Separation Problem (BSS), is that source signals are are assumed static. Clearly, this would not be the case for fault zone processes affecting timeseries, but Gualandi et al. (2015) show that the assumption is a fine approximation for mogi sources, seasonal signals, and post-seismic signals in GPS networks. They present subsequent papers detecting Slow Slip Events in the Cascades and Andes. My hope is to apply the same method to strainmeter data. \n",
    "\n",
    "vbICA uses a modelling approach (as opposed to a mapping approach, which something like FastICA, for example, uses) to explain signals in the data. In this approach, a contrast function, either the liklihood or, if Bayesian, the posterior probability distribution function (pdf), of the parameters is maximized. Gualandi et al. modify the code of Choudrey (2002) following Chan et al. (2003) to account for missing data. \n",
    "\n",
    "The generative model for this approach is characterized by observed variables (i.e. the data), hidden variables, and hidden parameters. Hidden parameters and variables are unknown. \n",
    "\n",
    "The results are highly dependent on the choice of priors. Priors enter the initial estimation of the weights used to characterize the generative model. Weights are composed of hidden/latent variables and hidden parameters. Hidden variables are identified with real world quantities. Hidden parameters, also termed hyper-parameters, are required for a working model. Specified priors include loosely constrained hyper-parameter values, with pdfs that describe the random variables and prior parameters. \n",
    "\n",
    "The bayesian application of this method involves automatic relevance determination (ARD). Giving weak confidence to priors allows data to guide the estimation of posterior parameters more heavily (as opposed to being governed by the priors). ARD essentially uses a precision value (the variance) of each source signal to determine the optimal number of components to explain the data. Gualandi et al. (2015) decide to call any signal with a maximum variance over 10x larger than the minimum variance noise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "from sklearn.decomposition import FastICA\n",
    "from scipy import signal\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.imaging import spectrogram\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox, interact, Layout\n",
    "style = {'description_width': 'initial'}\n",
    "layout=Layout(width='30%', height='40px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pick the file you would like to analyze:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab84548edc6f4c45b3a223dc5ce37f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Station file:', options=('PB.B916.T0.RS.2019-06-01',), value='PB.B916.T0.RS.2019-06-01')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5543ea7cee6a4c5fb135686f30b7fdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description='Load files to a dataframe', layout=Layout(height='40px', width='30%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa85519916c94370930316ea101db6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "# Assign station codes from selected file\n",
    "\n",
    "dir = './DataFiles/Level2/'\n",
    "sta_list = []\n",
    "for file in os.listdir(dir):\n",
    "    if file.endswith('regional_strain_and_corrections.txt'):\n",
    "        sta_list.append(file[0:24])\n",
    "        \n",
    "# Set initial values   \n",
    "file = sta_list[0]\n",
    "network = file[0:2]\n",
    "scode = file[3:7]\n",
    "loc = file[8:10]\n",
    "cha = file[11:13]\n",
    "\n",
    "print('Pick the file you would like to analyze:')\n",
    "sta_select = widgets.Dropdown(\n",
    "            options=sta_list,\n",
    "            value=sta_list[0],\n",
    "            description='Station file:',\n",
    "            )\n",
    "display(sta_select)\n",
    "\n",
    "# Change the station and network as the dropdown is changed\n",
    "def the_ccodes(siteval):\n",
    "    global scode, network, loc, cha, file\n",
    "    file = siteval\n",
    "    network = siteval[0:2]\n",
    "    scode = siteval[3:7]\n",
    "    loc = siteval[8:10]\n",
    "    cha = siteval[11:13]\n",
    "def on_cselect(change):\n",
    "    the_ccodes(change.new)\n",
    "sta_select.observe(on_cselect,names='value')\n",
    "\n",
    "# Load the strains and corrections to a dataframe\n",
    "\n",
    "# Make a dataframe with the file and assign start and end dates\n",
    "fbutton = widgets.Button(description=\"Load files to a dataframe\", button_style='danger',layout=layout)\n",
    "foutput = widgets.Output()\n",
    "\n",
    "# Initial start and end times (arbitrarily chosen)\n",
    "start = UTCDateTime('2000-01-01 00:00:00.000')\n",
    "end = UTCDateTime('2000-01-02 00:00:00.000')\n",
    "\n",
    "def on_fbutton_clicked(b):\n",
    "    with foutput:\n",
    "        foutput.clear_output()\n",
    "        global df, start, end\n",
    "        df = pd.DataFrame([])\n",
    "        for files in os.listdir(dir):\n",
    "            if files.startswith(file):\n",
    "                # Print file comments\n",
    "                with open(dir+files,'r') as f:\n",
    "                    for ln in f:\n",
    "                        if ln.startswith('#'):\n",
    "                            print('File comment: '+ ln[1:])\n",
    "                f.close()\n",
    "                adf = pd.read_csv(dir+files,sep='\\t',index_col=0,header=0,comment='#')\n",
    "                df = pd.concat([df,adf],axis='columns')\n",
    "        print('Wait for the dataframe column headers to print.')\n",
    "\n",
    "        ind = []\n",
    "        for i in range(0,len(df)):\n",
    "            ind.append(UTCDateTime(df.index[i]))\n",
    "        df.index = ind\n",
    "        start = df.index[0]\n",
    "        end = df.index[-1]\n",
    "        print(df.columns)\n",
    "fbutton.on_click(on_fbutton_clicked)\n",
    "\n",
    "\n",
    "display(fbutton, foutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the steps of Gualandi  et al. (2016) \"Blind source separation problem in GPS time series\"\n",
    "\n",
    "1. Center the dataset, i.e., remove the mean to each time\n",
    "series.\n",
    "> CH: This is simple\n",
    "2. Check the correlation of the centered dataset.\n",
    "> CH: This can be completed with the numpy pearson correlation coefficient tool of numpy.\n",
    "(2a) If the correlation is greater than 0.67, go to point 3).\n",
    "(2b) If the correlation is smaller than 0.67, go to point 4).\n",
    "3. Detrend the time series.\n",
    "> CH: The scipy detrend tool is good for this\n",
    "4. Correct for the co-seismic offsets because of the nonindependence\n",
    "with the post-seismic signals.\n",
    "5. Perform a vbICA with loose priors and select the number\n",
    "of components via a criterion based on the ARD method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ch0 [ms]', 'ch1 [ms]', 'ch2 [ms]', 'ch3 [ms]', 'baro_ch0', 'baro_ch1',\n",
       "       'baro_ch2', 'baro_ch3', 'trend_ch0', 'trend_ch1', 'trend_ch2',\n",
       "       'trend_ch3', 'tide_ch0', 'tide_ch1', 'tide_ch2', 'tide_ch3', 'gaugeEA',\n",
       "       'gaugeED', 'gaugeES', 'tideEA', 'tideED', 'tideES', 'baroEA', 'baroED',\n",
       "       'baroES', 'trendEA', 'trendED', 'trendES'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic test of FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.column_stack([df.tide_ch3.values+df.baro_ch3.values,df.tide_ch2.values+df.baro_ch2.values])\n",
    "time = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00201263, -0.00147288, -0.00084746, ..., -0.01109687,\n",
       "       -0.01054443, -0.00991671])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tide_ch3.values+df.baro_ch3.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4177, 2)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components=2, algorithm='parallel', whiten=True, fun='exp',max_iter=100000,tol=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered = ica.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01393687,  0.0094695 ],\n",
       "       [ 0.01432477,  0.01091599],\n",
       "       [ 0.01456449,  0.01238958],\n",
       "       ..., \n",
       "       [ 0.03142761,  0.00831115],\n",
       "       [ 0.03184684,  0.0098131 ],\n",
       "       [ 0.0321271 ,  0.0113304 ]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5546a24621f14db382cf8e2cd7101009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close()\n",
    "plt.plot(time,recovered)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient for ch0:  -0.996171195614\n",
      "Correlation greater than 0.67! Detrending all channels\r",
      "New correlation coefficient for ch0:  -7.53254986555e-15\n",
      "Correlation coefficient for ch1:  -0.986745304892\n",
      "Correlation greater than 0.67! Detrending all channels\r",
      "New correlation coefficient for ch1:  -3.59976043345e-16\n",
      "Correlation coefficient for ch2:  -0.991781270901\n",
      "Correlation greater than 0.67! Detrending all channels\r",
      "New correlation coefficient for ch2:  -2.48582132924e-15\n",
      "Correlation coefficient for ch3:  -0.817223078637\n",
      "Correlation greater than 0.67! Detrending all channels\r",
      "New correlation coefficient for ch3:  -5.19773453876e-16\n"
     ]
    }
   ],
   "source": [
    "# Demean the data and check the correlation coefficient\n",
    "\n",
    "# Convert UTCDateTime to seconds\n",
    "time = [obspy.UTCDateTime(df.index[i]).timestamp for i in range(0,len(df))]\n",
    "ch = {}; abscorr = {}\n",
    "for cha in ['ch0 [ms]','ch1 [ms]','ch2 [ms]','ch3 [ms]']:\n",
    "    demeaned = df[cha].values - np.mean(df[cha])\n",
    "    ch[cha[2:3]] = demeaned\n",
    "    abscorr[cha[2:3]] = abs(np.corrcoef(time,demeaned)[0,1])\n",
    "    print('Correlation coefficient for '+cha[0:3]+': ',np.corrcoef(time,demeaned)[0,1])\n",
    "    # Detrend if correlation is large for any channel\n",
    "    if max(abscorr.values()) > 0.67:\n",
    "        print('Correlation greater than 0.67! Detrending all channels',end='\\r')\n",
    "        ch[cha[2:3]] = signal.detrend(demeaned)\n",
    "        abscorr[cha[2:3]] = abs(np.corrcoef(time,ch[cha[2:3]])[0,1])\n",
    "        print('New correlation coefficient for '+cha[0:3]+': ',np.corrcoef(time,ch[cha[2:3]])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9dec76a8a214e0bbb1c914db74ab4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close()\n",
    "plt.hist(df._ch0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa05b7930b94b7584db89aa65a8aa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close()\n",
    "plt.plot(time,recovered)\n",
    "#plt.plot(time,df.tide_ch0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06aa47c1f98845849ca2c8d3a346dc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.close()\n",
    "plt.plot(time,df.gaugeEA)\n",
    "plt.plot(time,df.gaugeES)\n",
    "plt.plot(time,df.gaugeED)\n",
    "#plt.plot(time,df.tide_ch0)\n",
    "#plt.plot(time,df.baro_ch0)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ch0 [ms]', 'ch1 [ms]', 'ch2 [ms]', 'ch3 [ms]', 'baro_ch0', 'baro_ch1',\n",
       "       'baro_ch2', 'baro_ch3', 'trend_ch0', 'trend_ch1', 'trend_ch2',\n",
       "       'trend_ch3', 'tide_ch0', 'tide_ch1', 'tide_ch2', 'tide_ch3', 'gaugeEA',\n",
       "       'gaugeED', 'gaugeES', 'tideEA', 'tideED', 'tideES', 'baroEA', 'baroED',\n",
       "       'baroES', 'trendEA', 'trendED', 'trendES'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTCDateTime(df.index[0]),UTCDateTime(df.index[-1])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
